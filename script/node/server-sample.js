// 使用node server-sample.js可以快速起一个后端接口，保证正常展示。但数据都是错的！
var express = require("express");
var app = express();

app.get("/", function (req, res) {
  res.send("server-sample success!");
});

app.get("/hdfsbugdb-server/api/classify", function (req, res) {
  let data = {"code":"200","data":{"Quality":{"Availability":138,"Efficiency":28,"Reliability":118,"Security":30,"Compatibility":21,"Usability":46},"Consequence":{"Data-Loss":26,"Data-Corruption":18,"Test-Error":39,"Runtime-Error":49,"Build-Error":12,"Performance":34,"Failure":131,"Potential-Impact":72},"Component":{"HttpFS":10,"SnapShot":26,"WebHDFS":8,"Datanode":86,"Disk-Balancer":10,"LIBS":13,"FSCK":7,"Namenode":71,"RBF":31,"HA":20,"DOCS":11,"Client":55,"EC":22,"Cache":11},"Code":{"Error-Handle":24,"Config":42,"Concurrent":29,"Maintenance":108,"Logic":169,"Interface":9},"Significance":{"Vital":381,"Negligible":96}},"error":null}
  res.json(data);
});

app.get("/hdfsbugdb-server/api/issues/all/5/*", function (req, res) {
  let data ={"code":"200","data":{"total":381,"pageSize":5,"pageNum":1,"sum":5,"data":[{"Status":"Resolved","Impact":"标准化跨平台使用OpenSSL失败","CreatedTime":"04/Nov/21 12:41","IssueType":"Bug","Priority":"Blocker","Consequence":"Build-Error","Code":"Maintenance","Project":"Hadoop HDFS","Quality":"Compatibility","Cause":"OpenSSL库从OpenSSL版本1.1.0开始从eay32重命名为libcrypto","UpdateTime":"07/Nov/21 18:47","IssueKey":"HDFS-16300","Summary":"Use libcrypto in Windows for libhdfspp","id":1,"Component":"LIBS","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"如果打开 processCommand 异步功能,Namenode 重新启动时缺少 IBR,即Namenode重新启动后丢失块","CreatedTime":"11/Jan/20 11:49","IssueType":"Bug","Priority":"Blocker","Consequence":"Data-Loss","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"Datanode的块汇报分为IBR(增量块汇报)和FBR(全量块汇报),Namenode重启的时候,会发送 DNA_REGISTER 命令给Datanode,Datanode收到后,异步运行#reRegister,由于是异步运行,无法确定发送FBR和清除IBR哪个先运行。如果先发送FBR然后清除IBR,就会在这两个时间点之间丢失一些块,直到下一个FBR。","UpdateTime":"22/Sep/21 10:31","IssueKey":"HDFS-15113","Summary":"Missing IBR when NameNode restart if open processCommand async feature","id":2,"Component":"Datanode","Significance":"Vital","Link":"HDFS-14997","Resolution":"Fixed"},{"Status":"Resolved","Impact":"ec重建失败,块丢失","CreatedTime":"25/Mar/20 08:19","IssueType":"Bug","Priority":"Blocker","Consequence":"Data-Loss","Code":"Logic","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"Dirty buffer 导致重建块失败,Dirty buffer的出现是因为在释放buffer后,StripedBlockReader仍旧保留buffer并且写入内容,导致污染了BufferPool","UpdateTime":"13/Sep/21 07:33","IssueKey":"HDFS-15240","Summary":"Erasure Coding: dirty buffer causes reconstruction block error","id":3,"Component":"EC","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"Namenode无法自行退出安全模式,该模式下Namenode不接收任何对于命名空间的修改操作,同时也不触发任何复制和删除数据块的操作","CreatedTime":"19/Jun/20 14:31","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"在HDFS-14941之后,global gen stamp的更新在某些情况下被延迟,这使得最后一组增量块报告泄露","UpdateTime":"10/Jun/21 07:43","IssueKey":"HDFS-15421","Summary":"IBR leak causes standby NN to be stuck in safe mode","id":4,"Component":"Namenode","Significance":"Vital","Link":"HDFS-14941","Resolution":"Fixed"},{"Status":"Resolved","Impact":"破坏了 3.3.0 和 3.3.1 两个版本之间的源代码兼容性。","CreatedTime":"17/May/21 09:07","IssueType":"Bug","Priority":"Blocker","Consequence":"Build-Error","Code":"Interface","Project":"Hadoop HDFS","Quality":"Compatibility","Cause":"JournalNodeMXBean是一个公共接口,在某次更新中,添加了getClusterIds()、getClusterIds()、getVersion()三个方法,","UpdateTime":"19/May/21 02:49","IssueKey":"HDFS-16027","Summary":" HDFS-15245 breaks source code compatibility between 3.3.0 and 3.3.1.","id":5,"Component":"HA","Significance":"Vital","Link":"HDFS-15245","Resolution":"Fixed"}]},"error":null}
  res.json(data);
});

app.get("/hdfsbugdb-server/api/issues/all/15/*", function (req, res) {
    let data = {"code":"200","data":{"total":381,"pageSize":15,"pageNum":1,"sum":15,"data":[{"Status":"Resolved","Impact":"标准化跨平台使用OpenSSL失败","CreatedTime":"04/Nov/21 12:41","IssueType":"Bug","Priority":"Blocker","Consequence":"Build-Error","Code":"Maintenance","Project":"Hadoop HDFS","Quality":"Compatibility","Cause":"OpenSSL库从OpenSSL版本1.1.0开始从eay32重命名为libcrypto","UpdateTime":"07/Nov/21 18:47","IssueKey":"HDFS-16300","Summary":"Use libcrypto in Windows for libhdfspp","id":1,"Component":"LIBS","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"如果打开 processCommand 异步功能,Namenode 重新启动时缺少 IBR,即Namenode重新启动后丢失块","CreatedTime":"11/Jan/20 11:49","IssueType":"Bug","Priority":"Blocker","Consequence":"Data-Loss","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"Datanode的块汇报分为IBR(增量块汇报)和FBR(全量块汇报),Namenode重启的时候,会发送 DNA_REGISTER 命令给Datanode,Datanode收到后,异步运行#reRegister,由于是异步运行,无法确定发送FBR和清除IBR哪个先运行。如果先发送FBR然后清除IBR,就会在这两个时间点之间丢失一些块,直到下一个FBR。","UpdateTime":"22/Sep/21 10:31","IssueKey":"HDFS-15113","Summary":"Missing IBR when NameNode restart if open processCommand async feature","id":2,"Component":"Datanode","Significance":"Vital","Link":"HDFS-14997","Resolution":"Fixed"},{"Status":"Resolved","Impact":"ec重建失败,块丢失","CreatedTime":"25/Mar/20 08:19","IssueType":"Bug","Priority":"Blocker","Consequence":"Data-Loss","Code":"Logic","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"Dirty buffer 导致重建块失败,Dirty buffer的出现是因为在释放buffer后,StripedBlockReader仍旧保留buffer并且写入内容,导致污染了BufferPool","UpdateTime":"13/Sep/21 07:33","IssueKey":"HDFS-15240","Summary":"Erasure Coding: dirty buffer causes reconstruction block error","id":3,"Component":"EC","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"Namenode无法自行退出安全模式,该模式下Namenode不接收任何对于命名空间的修改操作,同时也不触发任何复制和删除数据块的操作","CreatedTime":"19/Jun/20 14:31","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"在HDFS-14941之后,global gen stamp的更新在某些情况下被延迟,这使得最后一组增量块报告泄露","UpdateTime":"10/Jun/21 07:43","IssueKey":"HDFS-15421","Summary":"IBR leak causes standby NN to be stuck in safe mode","id":4,"Component":"Namenode","Significance":"Vital","Link":"HDFS-14941","Resolution":"Fixed"},{"Status":"Resolved","Impact":"破坏了 3.3.0 和 3.3.1 两个版本之间的源代码兼容性。","CreatedTime":"17/May/21 09:07","IssueType":"Bug","Priority":"Blocker","Consequence":"Build-Error","Code":"Interface","Project":"Hadoop HDFS","Quality":"Compatibility","Cause":"JournalNodeMXBean是一个公共接口,在某次更新中,添加了getClusterIds()、getClusterIds()、getVersion()三个方法,","UpdateTime":"19/May/21 02:49","IssueKey":"HDFS-16027","Summary":" HDFS-15245 breaks source code compatibility between 3.3.0 and 3.3.1.","id":5,"Component":"HA","Significance":"Vital","Link":"HDFS-15245","Resolution":"Fixed"},{"Status":"Resolved","Impact":"edits version太低,需要升级","CreatedTime":"28/Apr/21 22:55","IssueType":"Bug","Priority":"Blocker","Consequence":"Test-Error","Code":"Config","Project":"Hadoop HDFS","Quality":"Availability","Cause":"TestOfflineEditsViewer.testStored()读取FSEditLogOpCodes的负值失败。","UpdateTime":"08/May/21 05:32","IssueKey":"HDFS-16001","Summary":"TestOfflineEditsViewer.testStored() fails reading negative value of FSEditLogOpCodes","id":6,"Component":"HA","Significance":"Vital","Link":"HDFS-15566","Resolution":"Fixed"},{"Status":"Resolved","Impact":"Namenode重新启动失败","CreatedTime":"09/Sep/20 17:45","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Availability","Cause":"更新中向editLog事务引入了modification time,当NN重启的时候,从editLog文件中读取旧的布局版本,且假设事务也来自先前的版本,因此跳过解析modification time,会级联导致NN关闭.","UpdateTime":"03/May/21 18:04","IssueKey":"HDFS-15566","Summary":"NN restart fails after RollingUpgrade from  3.1.3/3.2.1 to 3.3.0","id":7,"Component":"Namenode","Significance":"Vital","Link":"HDFS-14922、HDFS-14924、HDFS-15054","Resolution":"Fixed"},{"Status":"Resolved","Impact":"Namenode启动失败","CreatedTime":"04/Mar/20 01:39","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Availability","Cause":"加载FSImage时,FileSummary的排序方法错误,导致加载FSImage失败","UpdateTime":"23/Apr/20 07:02","IssueKey":"HDFS-15205","Summary":"FSImage sort section logic is wrong","id":8,"Component":"Namenode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"造成严重的性能下降","CreatedTime":"08/Apr/20 00:46","IssueType":"Bug","Priority":"Blocker","Consequence":"Performance","Code":"Logic","Project":"Hadoop HDFS","Quality":"Efficiency","Cause":"Namenode初始化的时候检查多次授权API版本","UpdateTime":"09/Apr/20 17:46","IssueKey":"HDFS-15269","Summary":"NameNode should check the authorization API version only once during initialization","id":9,"Component":"Namenode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"旧的实现无法编译","CreatedTime":"23/Mar/20 20:18","IssueType":"Bug","Priority":"Blocker","Consequence":"Build-Error","Code":"Interface","Project":"Hadoop HDFS","Quality":"Compatibility","Cause":"新的API INodeAttributeProvider#checkPermissionWithContext()缺少方法体","UpdateTime":"25/Mar/20 23:17","IssueKey":"HDFS-15234","Summary":"Add a default method body for the INodeAttributeProvider#checkPermissionWithContext API","id":10,"Component":"Namenode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"Namenode解析EditLog失败","CreatedTime":"25/Nov/19 23:23","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Availability","Cause":"HDFS-13101引入的代码逻辑错误,当删除大量快照后,解析EditLog失败","UpdateTime":"18/Dec/19 18:12","IssueKey":"HDFS-15012","Summary":"NN fails to parse Edit logs after applying HDFS-13101","id":11,"Component":"SnapShot","Significance":"Vital","Link":"HDFS-13101","Resolution":"Fixed"},{"Status":"Resolved","Impact":"重命名快照失败","CreatedTime":"17/Oct/19 16:35","IssueType":"Bug","Priority":"Blocker","Consequence":"Data-Loss","Code":"Maintenance","Project":"Hadoop HDFS","Quality":"Availability","Cause":"The this.removeFeature(..) is getting called two times in InodeDirectory.java.","UpdateTime":"24/Oct/19 20:26","IssueKey":"HDFS-14910","Summary":"Rename Snapshot with Pre Descendants Fail With IllegalArgumentException.","id":12,"Component":"SnapShot","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"HDFS NameNode and JournalNode 在windows上启动失败","CreatedTime":"03/Oct/19 16:49","IssueType":"Bug","Priority":"Blocker","Consequence":"Runtime-Error","Code":"Logic","Project":"Hadoop HDFS","Quality":"Security","Cause":"在windows上,设置Name directory权限失败","UpdateTime":"15/Oct/19 18:45","IssueKey":"HDFS-14890","Summary":"Setting permissions on name directory fails on non posix compliant filesystems","id":13,"Component":"Namenode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"DN计算出错误的密码","CreatedTime":"23/May/19 12:42","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Security","Cause":"在升级集群2.X到3.X的时候,需要先升级NN,此时出现NN3.X,但DN2.X的情况,由于代码逻辑的问题,如果 NN 的标识符添加新字段,DN 将丢失字段并计算错误的密码。","UpdateTime":"10/Oct/19 20:31","IssueKey":"HDFS-14509","Summary":"DN throws InvalidToken due to inequality of password when upgrade NN 2.x to 3.x","id":14,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"从 Hadoop 2.9.0/2.8.3/3.0.0 升级到 3.0.1/3.1.0 及更高版本时,这可能会导致问题。","CreatedTime":"21/Jun/19 16:02","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Interface","Project":"Hadoop HDFS","Quality":"Compatibility","Cause":"HDFS-11848向 DistributedFileSystem.listOpenFiles() 添加了一个附加参数,但它不保留现有 API。","UpdateTime":"02/Oct/19 17:15","IssueKey":"HDFS-14595","Summary":"HDFS-11848 breaks API compatibility","id":15,"Component":"Client","Significance":"Vital","Link":"11848","Resolution":"Fixed"}]},"error":null}
    res.json(data);
  });
  

app.get("/hdfsbugdb-server/api/issues/search/*", function (req, res) {
    let data  = {"code":"200","data":{"total":86,"pageSize":15,"pageNum":1,"sum":15,"data":[{"Status":"Resolved","Impact":"如果打开 processCommand 异步功能,Namenode 重新启动时缺少 IBR,即Namenode重新启动后丢失块","CreatedTime":"11/Jan/20 11:49","IssueType":"Bug","Priority":"Blocker","Consequence":"Data-Loss","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"Datanode的块汇报分为IBR(增量块汇报)和FBR(全量块汇报),Namenode重启的时候,会发送 DNA_REGISTER 命令给Datanode,Datanode收到后,异步运行#reRegister,由于是异步运行,无法确定发送FBR和清除IBR哪个先运行。如果先发送FBR然后清除IBR,就会在这两个时间点之间丢失一些块,直到下一个FBR。","UpdateTime":"22/Sep/21 10:31","IssueKey":"HDFS-15113","Summary":"Missing IBR when NameNode restart if open processCommand async feature","id":2,"Component":"Datanode","Significance":"Vital","Link":"HDFS-14997","Resolution":"Fixed"},{"Status":"Resolved","Impact":"DN计算出错误的密码","CreatedTime":"23/May/19 12:42","IssueType":"Bug","Priority":"Blocker","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Security","Cause":"在升级集群2.X到3.X的时候,需要先升级NN,此时出现NN3.X,但DN2.X的情况,由于代码逻辑的问题,如果 NN 的标识符添加新字段,DN 将丢失字段并计算错误的密码。","UpdateTime":"10/Oct/19 20:31","IssueKey":"HDFS-14509","Summary":"DN throws InvalidToken due to inequality of password when upgrade NN 2.x to 3.x","id":14,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"试图移除卷的线程等待并陷入死循环,同时由于线程在移除卷时一直持有 checkDirsLock,其他试图获取相同锁的线程将被永久阻塞。","CreatedTime":"10/Apr/21 15:32","IssueType":"Bug","Priority":"Critical","Consequence":"Failure","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"当 BlockSender 因找不到元数据而抛出异常时,线程获取的卷引用没有被释放。","UpdateTime":"18/Nov/21 07:46","IssueKey":"HDFS-15963","Summary":"Unreleased volume references cause an infinite loop","id":24,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"块损坏","CreatedTime":"19/Jun/20 15:06","IssueType":"Bug","Priority":"Critical","Consequence":"Data-Corruption","Code":"Logic","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"当在备用Namenode上排队 IBR（增量块报告）时，一些报告的信息将被现有存储的信息替换","UpdateTime":"10/Jun/21 07:44","IssueKey":"HDFS-15422","Summary":"Reported IBR is partially replaced with stored info when queuing.","id":33,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"调用 hdfs dfsadmin -refreshNamenodes hostname:50020 在联邦环境中注册一个新的命名空间时，DataNode可能会死锁","CreatedTime":"19/Oct/20 12:28","IssueType":"Bug","Priority":"Critical","Consequence":"Failure","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Reliability","Cause":" BPServiceActor 中的线程启动顺序有误，无法保证在启动bpThread之前，lifelineSender已经获取并释放了锁","UpdateTime":"10/Jun/21 07:44","IssueKey":"HDFS-15641","Summary":"DataNode could meet deadlock if invoke refreshNameNode","id":34,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"单元测试无法清理测试数据并导致未来的 Maven 测试运行崩溃","CreatedTime":"17/Jan/20 01:41","IssueType":"Bug","Priority":"Critical","Consequence":"Test-Error","Code":"Maintenance","Project":"Hadoop HDFS","Quality":"Availability","Cause":"TestDataNodeVolumeFailureToleration 失败","UpdateTime":"27/Jan/20 03:49","IssueKey":"HDFS-15128","Summary":"Unit test failing to clean testing data and crashed future Maven test run due to failure in TestDataNodeVolumeFailureToleration","id":48,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"字段timedOutItems同步错误","CreatedTime":"28/Jun/19 18:50","IssueType":"Bug","Priority":"Critical","Consequence":"Failure","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Security","Cause":"字段timedOutItems(ArrayList类型，非线程安全)，受自身同步保护(timedOutItems)，但是在另一个地方它（试图）通过使用pendingReconstructions进行同步保护——但这不能保护timedOutItems。在不同对象上同步并不能确保与其他位置互斥。","UpdateTime":"02/Oct/19 22:09","IssueKey":"HDFS-14618","Summary":"Incorrect synchronization of ArrayList field (ArrayList is thread-unsafe).","id":54,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"HashMap线程安全问题","CreatedTime":"26/Jun/19 00:00","IssueType":"Bug","Priority":"Critical","Consequence":"Failure","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Security","Cause":"HashMap 不是线程安全的。字段 storageMap 通常由 storageMap 同步。但是，在一个地方，字段 storageMap 不受同步保护。","UpdateTime":"02/Oct/19 21:38","IssueKey":"HDFS-14610","Summary":"HashMap is not thread safe. Field storageMap is typically synchronized by storageMap. However, in one place, field storageMap is not protected with synchronized.","id":55,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"异常的块文件会导致计算空间异常","CreatedTime":"11/Nov/21 07:19","IssueType":"Bug","Priority":"Major","Consequence":"Data-Corruption","Code":"Maintenance","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"DirectoryScanner需要改进，添加常规文件检查相关块。异常的块文件会导致计算空间异常，它们应该被主动识别和过滤，有利于集群的稳定性","UpdateTime":"22/Feb/22 02:16","IssueKey":"HDFS-16316","Summary":"Improve DirectoryScanner: add regular file check related block","id":60,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"导致 DatanodeDescriptor 被两次添加到 pendingNodes 队列中。","CreatedTime":"28/Jan/22 15:10","IssueType":"Bug","Priority":"Major","Consequence":"Data-Corruption","Code":"Error-Handle","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"DatanodeAdminDefaultMonitor 在异常时双重排队 DatanodeDescriptor 的边缘情况","UpdateTime":"31/Jan/22 07:17","IssueKey":"HDFS-16443","Summary":"Fix edge case where DatanodeAdminDefaultMonitor doubly enqueues a DatanodeDescriptor on exception","id":67,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"如果在给定时间有超过 100 个数据节点被停用，存在一种边缘情况，该逻辑会阻止Namenode处理退役，既然该100个节点无法退役，那么其他节点也永远无法退役。","CreatedTime":"05/Nov/21 17:06","IssueType":"Bug","Priority":"Major","Consequence":"Failure","Code":"Config","Project":"Hadoop HDFS","Quality":"Availability","Cause":"HDFS Namenode 类“DatanodeAdminManager”负责停用数据节点,根据这个“hdfs-site”配置：dfs.namenode.decommission.max.concurrent.tracked.nodes(默认为100），Namenode 在任何给定时间只会主动跟踪最多 100 个数据节点以进行退役，以避免 Namenode 内存压力。","UpdateTime":"31/Jan/22 07:13","IssueKey":"HDFS-16303","Summary":"Losing over 100 datanodes in state decommissioning results in full blockage of all datanode decommissioning","id":68,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"目前，即使手动触发 BR 或因 diskError 触发 BR，也会重置 BlockReport 间隔。","CreatedTime":"13/Feb/20 08:14","IssueType":"Bug","Priority":"Major","Consequence":"Performance","Code":"Logic","Project":"Hadoop HDFS","Quality":"Efficiency","Cause":"除了第一个块报告之外，不应重置块报告间隔","UpdateTime":"20/Jan/22 13:16","IssueKey":"HDFS-15167","Summary":"Block Report Interval shouldn't be reset apart from first Block Report","id":71,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"打的日志抛出的异常消息晦涩","CreatedTime":"10/Dec/21 11:22","IssueType":"Bug","Priority":"Major","Consequence":"Runtime-Error","Code":"Error-Handle","Project":"Hadoop HDFS","Quality":"Reliability","Cause":"客户端和上游Datanode 在访问 FsDatasetSpi 之前没有 CheckNotNull，因此仅仅知道没有初始化，而不知道nullpointexception的具体原因","UpdateTime":"04/Jan/22 22:43","IssueKey":"HDFS-16377","Summary":"Should CheckNotNull before access FsDatasetSpi","id":76,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"1.导致MetadataOperationRateAvgTime在某些情况下非常大2.导致 Namenode 端出现慢速磁盘指标错误。","CreatedTime":"09/Nov/21 12:22","IssueType":"Bug","Priority":"Major","Consequence":"Failure","Code":"Logic","Project":"Hadoop HDFS","Quality":"Availability","Cause":"DataNodeVolumeMetrics 中的 Metric metadataOperationRate 计算错误","UpdateTime":"07/Dec/21 07:24","IssueKey":"HDFS-16311","Summary":"Metric metadataOperationRate calculation error in DataNodeVolumeMetrics","id":80,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"},{"Status":"Resolved","Impact":"调用 dfsadmin -reconfig datanode ip:host start 触发#refreshVolumes 时，DataNode 可能会遇到死锁。","CreatedTime":"17/Dec/19 03:57","IssueType":"Bug","Priority":"Major","Consequence":"Failure","Code":"Concurrent","Project":"Hadoop HDFS","Quality":"Availability","Cause":"datanode锁的错误使用","UpdateTime":"29/Nov/21 08:11","IssueKey":"HDFS-15068","Summary":"DataNode could meet deadlock if invoke refreshVolumes when register","id":83,"Component":"Datanode","Significance":"Vital","Link":"","Resolution":"Fixed"}]},"error":null}
    res.json(data)
});



var server = app.listen(8081, function () {
  var host = server.address().address;
  var port = server.address().port;

  console.log("应用实例，访问地址为 http://localhost:%s", port);
});
